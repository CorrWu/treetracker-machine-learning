{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/shubhomb/greenstand_data_analysis/blob/master/imnet/tree_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hV4G09d8nrxd"
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/googlecolab/colabtools/blob/master/notebooks/colab-github-demo.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
<<<<<<< HEAD
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "SwiQQCjdvnqJ",
    "outputId": "2fb4d5c2-e2bb-45c6-be42-ad0da7d7cc78"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-f04af2c8ce59>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# how to access GDrive https://colab.research.google.com/notebooks/io.ipynb#scrollTo=RWSJpsyKqHjH\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mgdir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"drive\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"My Drive\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
=======
      "name": "tree_detection.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9e1f88b442454b538d64c9cff7505411": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_fb3c6a1d4edc4c78b1b2ad823838bbf1",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_e7bee5115bcc4142b743981a0011bdf4",
              "IPY_MODEL_05125a8cfbb1478ea40a0772830b4677"
            ]
          }
        },
        "fb3c6a1d4edc4c78b1b2ad823838bbf1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e7bee5115bcc4142b743981a0011bdf4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_d86ed8e90eda4e4a90d429232fc9e62a",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 14212972,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 14212972,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d217d5c345d840c58404121e1f5257e9"
          }
        },
        "05125a8cfbb1478ea40a0772830b4677": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_04f548f0be6b461e9252fb102896eea0",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 13.6M/13.6M [00:00&lt;00:00, 63.0MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f62c4ea6649a45bf88564acba82add1a"
          }
        },
        "d86ed8e90eda4e4a90d429232fc9e62a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d217d5c345d840c58404121e1f5257e9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "04f548f0be6b461e9252fb102896eea0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f62c4ea6649a45bf88564acba82add1a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
>>>>>>> 26039cbb9be4bed73ea4ced92991de85923ab12d
    }
   ],
   "source": [
    "\n",
    "# how to access GDrive https://colab.research.google.com/notebooks/io.ipynb#scrollTo=RWSJpsyKqHjH\n",
    "from google.colab import files, drive\n",
    "import os\n",
    "drive.mount('/content/drive')\n",
    "gdir = os.path.join(os.getcwd(), \"drive\", \"My Drive\")\n"
   ]
  },
<<<<<<< HEAD
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QoKegeKl28VU"
   },
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
=======
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shubhomb/greenstand_data_analysis/blob/master/imnet/tree_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hV4G09d8nrxd"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/googlecolab/colabtools/blob/master/notebooks/colab-github-demo.ipynb)"
      ]
>>>>>>> 26039cbb9be4bed73ea4ced92991de85923ab12d
    },
    "colab_type": "code",
    "id": "MojfFLIl2-En",
    "outputId": "1dadfcb4-6e7c-4f27-a84c-3cb3fea3de2c"
   },
   "outputs": [
    {
<<<<<<< HEAD
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Torch Dataset and IMNet Loading\n",
    "import torch\n",
    "from xml.etree import ElementTree\n",
    "from torch.utils import data\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torchvision import transforms\n",
    "from PIL import Image, ImageDraw\n",
    "from  collections import OrderedDict\n",
    "import numpy as np\n",
    "\n",
    "# Model development and training\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "\n",
    "# Filesystem and parallelization\n",
    "import os\n",
    "import multiprocessing\n",
    "\n",
    "# Utility \n",
    "import time\n",
    "import datetime\n",
    "\n",
    "# Constants for parallelization\n",
    "_DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# from ImageNet\n",
    "synsets = {\n",
    "    \"judas\": \"n12513613\",\n",
    "    \"palm\": \"n12582231\",\n",
    "    \"pine\": \"n11608250\",\n",
    "    \"china tree\": \"n12741792\",\n",
    "    \"fig\": \"n12401684\",\n",
    "    \"cabbage\": \"n12478768\",\n",
    "    \"cacao\": \"n12201580\",\n",
    "    \"kapok\": \"n12190410\",\n",
    "    \"iron\": \"n12317296\",\n",
    "    \"linden\": \"n12202936\",\n",
    "    \"pepper\": \"n12765115\",\n",
    "    \"rain\": \"n11759853\",\n",
    "    \"dita\": \"n11770256\",\n",
    "    \"alder\": \"n12284262\",\n",
    "    \"silk\": \"n11759404\",\n",
    "    \"coral\": \"n12527738\",\n",
    "    \"huisache\": \"n11757851\",\n",
    "    \"fringe\": \"n12302071\",\n",
    "    \"dogwood\": \"n12946849\",\n",
    "    \"cork\": \"n12713866\",\n",
    "    \"ginkgo\": \"n11664418\",\n",
    "    \"golden shower\": \"n12492106\",\n",
    "    \"balata\": \"n12774299\",\n",
    "    \"baobab\": \"n12189987\",\n",
    "    \"sorrel\": \"n12242409\",\n",
    "    \"Japanese pagoda\": \"n12570394\",\n",
    "    \"Kentucky coffee\": \"n12496427\",\n",
    "    \"Logwood\": \"n12496949\"\n",
    "}\n",
    "\n",
    "\n",
    "print (\"Device:\" , _DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Wshbg0pg3IHX"
   },
   "source": [
    "\n",
    "## Dataset Creation\n",
    "Define datasets for ImageNet and Greenstand sources. Greenstand species classes are yet unlabeled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LJ2MNziQ3B-B"
   },
   "outputs": [],
   "source": [
    "class ImnetDataset(data.Dataset):\n",
    "    \n",
    "    # initialise function of class\n",
    "    def __init__(self, dir, synsets, transforms=None, device=None, one_hot=False):\n",
    "        # the data directories\n",
    "        self.img_dir = os.path.join(dir, \"original_images\")\n",
    "        self.bb_dir = os.path.join(dir, \"bounding_boxes\")\n",
    "\n",
    "        #synsets library to get the associated class\n",
    "        self.synsets = synsets\n",
    "        self.rev_synsets = {y:x for x,y in zip(synsets.keys(), synsets.values())}\n",
    "        self.classes = list(self.synsets.keys())\n",
    "        # self.target = target\n",
    "\n",
    "        self.one_hot = one_hot\n",
    "        self.imgs = []\n",
    "\n",
    "        for i in self.classes:\n",
    "          temp_imgs = list(sorted(os.listdir(os.path.join(self.img_dir, i))))\n",
    "          for img_path in temp_imgs:\n",
    "            #in every directory the \"tar\" file is still present\n",
    "            if not \"tar\" in img_path:\n",
    "              name = img_path.split('.')[0]\n",
    "              self.imgs.append(name)\n",
    "\n",
    "        self.bb_dict = {}\n",
    "        for f, _, d in os.walk(self.bb_dir):\n",
    "          for file in d:\n",
    "            if os.path.splitext(file)[1] == \".xml\":\n",
    "              tree = ElementTree.parse(os.path.join(f, file))\n",
    "              root = tree.getroot()\n",
    "              obj = root.find(\"object\")\n",
    "              b = obj.find(\"bndbox\")\n",
    "              xmin = int(b.find(\"xmin\").text)\n",
    "              ymin = int(b.find(\"ymin\").text)\n",
    "              xmax = int(b.find(\"xmax\").text)\n",
    "              ymax = int(b.find(\"ymax\").text)\n",
    "              self.bb_dict[os.path.join(f, file)] =  (xmin, ymin, xmax, ymax)\n",
    "\n",
    "        self.transforms = transforms\n",
    "        self.device = device\n",
    "        \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        name = self.imgs[idx]\n",
    "        label = self.rev_synsets[name.split(\"_\")[0]]\n",
    "\n",
    "        img_path = os.path.join(self.img_dir, label, f\"{name}.JPEG\")\n",
    "        bb_path = os.path.join(self.bb_dir, label, \"Annotation\", name.split(\"_\")[0], f\"{name}.xml\")\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "\n",
    "        if bb_path in self.bb_dict.keys():\n",
    "          xmin, ymin, xmax, ymax = self.bb_dict[bb_path]\n",
    "        else:\n",
    "          # the whole image is the bounding box label, as NoneType was causing collating issue. \n",
    "          xmin = 0\n",
    "          ymin = 0\n",
    "          xmax = img.size[0]\n",
    "          ymax = img.size[1]\n",
    "        boxes = torch.as_tensor([xmin, ymin, xmax, ymax], dtype=torch.float32)\n",
    "\n",
    "        \n",
    "        # img, boxes = self.expand(img, [xmin, ymin, xmax, ymax], self.target, (0, 0, 0))\n",
    "        if self.transforms is not None:\n",
    "          img = self.transforms(img)\n",
    "\n",
    "        if self.one_hot: \n",
    "          image_id = torch.zeros(len(self.classes), dtype=torch.float32)\n",
    "          image_id [self.classes.index(label)] = 1.0\n",
    "        else:\n",
    "          image_id = torch.tensor([self.classes.index(label)])\n",
    "\n",
    "        targets = {}\n",
    "        targets[\"boxes\"] = boxes\n",
    "        targets[\"image_class\"] = image_id\n",
    "\n",
    "    \n",
    "        return img, targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QOicVbPgZrK1"
   },
   "outputs": [],
   "source": [
    "class GreenstandDataset(data.Dataset):\n",
    "  # We don't have labels for this yet...\n",
    "  def __init__(self, dir, device, transforms, bb_dir=None, one_hot=False):\n",
    "    self.img_dir = dir\n",
    "    self.imgs = []\n",
    "    self.bb_dir = bb_dir\n",
    "    self.transforms = transforms\n",
    "    self.classes = None # Change this when we define Greenstand class labels\n",
    "    self.one_hot = one_hot\n",
    "    self.device = device\n",
    "\n",
    "    for f, _, d in os.walk(test_path):\n",
    "      for fil in d:\n",
    "        if \"jpg\" in fil:\n",
    "          self.imgs.append(os.path.join(f, fil))\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    img = Image.open(self.img_dir[idx])\n",
    "    if self.transforms is not None:\n",
    "      img = self.transforms (img)\n",
    "\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bAGG9gAcEgdv"
   },
   "source": [
    "## MobileNet-v2\n",
    "See [the original paper](https://arxiv.org/pdf/1704.04861.pdf) for details. This was chosen first because Torchvision has pretrained weights and the net is quite low-latency, which may be useful for user-interface image selection. First go is to simply change the output layer to predict 4 coordinates for the bounding box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 106,
     "referenced_widgets": [
      "21a488cfd67f40a08af10fabf4ca8e48",
      "9b87637d692043f888a16d0912f7c900",
      "85e4de55e35e42b0a3ebcb2b7a8f5411",
      "79cd02a9290d4913a590c1812da0b714",
      "d4e996e48eac4bb191412a29a46388e3",
      "f2251329df3b41db8c6e5907a4c02d05",
      "37dd780c4fba4d66a78d06ee8b1183de",
      "70755204654046b997b9d312154ef497"
     ]
=======
      "cell_type": "code",
      "metadata": {
        "id": "SwiQQCjdvnqJ",
        "outputId": "96bbd659-5531-45ba-e81b-721c18752b3b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "\n",
        "# how to access GDrive https://colab.research.google.com/notebooks/io.ipynb#scrollTo=RWSJpsyKqHjH\n",
        "from google.colab import files, drive\n",
        "import os\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "gdir = os.path.join(os.getcwd(), \"drive\", \"My Drive\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
>>>>>>> 26039cbb9be4bed73ea4ced92991de85923ab12d
    },
    "colab_type": "code",
    "id": "5g48vTx64Wdz",
    "outputId": "a2fd5360-ac3f-480d-c14a-4dc6c3e627ae"
   },
   "outputs": [
    {
<<<<<<< HEAD
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/mobilenet_v2-b0353104.pth\" to /root/.cache/torch/hub/checkpoints/mobilenet_v2-b0353104.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21a488cfd67f40a08af10fabf4ca8e48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=14212972.0), HTML(value='')))"
=======
      "cell_type": "markdown",
      "metadata": {
        "id": "QoKegeKl28VU"
      },
      "source": [
        "## Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MojfFLIl2-En",
        "outputId": "50e9c562-3011-4ffe-ca0a-4f7f2baba910",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "# Torch Dataset and IMNet Loading\n",
        "import torch\n",
        "from xml.etree import ElementTree\n",
        "from torch.utils import data\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "from torchvision import transforms\n",
        "from PIL import Image, ImageDraw\n",
        "from  collections import OrderedDict\n",
        "import numpy as np\n",
        "\n",
        "# Model development and training\n",
        "import torchvision.models as models\n",
        "import torch.nn as nn\n",
        "\n",
        "# Filesystem and parallelization\n",
        "import os\n",
        "import multiprocessing\n",
        "\n",
        "# Utility \n",
        "import time\n",
        "import datetime\n",
        "\n",
        "# Constants for parallelization\n",
        "_DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "# from ImageNet\n",
        "tree_synsets = {\n",
        "    \"judas\": \"n12513613\",\n",
        "    \"palm\": \"n12582231\",\n",
        "    \"pine\": \"n11608250\",\n",
        "    \"china tree\": \"n12741792\",\n",
        "    \"fig\": \"n12401684\",\n",
        "    \"cabbage\": \"n12478768\",\n",
        "    \"cacao\": \"n12201580\",\n",
        "    \"kapok\": \"n12190410\",\n",
        "    \"iron\": \"n12317296\",\n",
        "    \"linden\": \"n12202936\",\n",
        "    \"pepper\": \"n12765115\",\n",
        "    \"rain\": \"n11759853\",\n",
        "    \"dita\": \"n11770256\",\n",
        "    \"alder\": \"n12284262\",\n",
        "    \"silk\": \"n11759404\",\n",
        "    \"coral\": \"n12527738\",\n",
        "    \"huisache\": \"n11757851\",\n",
        "    \"fringe\": \"n12302071\",\n",
        "    \"dogwood\": \"n12946849\",\n",
        "    \"cork\": \"n12713866\",\n",
        "    \"ginkgo\": \"n11664418\",\n",
        "    \"golden shower\": \"n12492106\",\n",
        "    \"balata\": \"n12774299\",\n",
        "    \"baobab\": \"n12189987\",\n",
        "    \"sorrel\": \"n12242409\",\n",
        "    \"Japanese pagoda\": \"n12570394\",\n",
        "    \"Kentucky coffee\": \"n12496427\",\n",
        "    \"Logwood\": \"n12496949\"\n",
        "}\n",
        "nontree_synsets = {\n",
        "   # Nontrees\n",
        "    \"garbage_bin\": \"n02747177\",\n",
        "    \"carion_fungus\": \"n13040303\",\n",
        "    \"basidiomycetous_fungus\": \"n13049953\",\n",
        "    \"jelly_fungus\": \"n13060190\",\n",
        "    \"desktop_computer\": \"n03180011\",\n",
        "    \"laptop_computer\": \"n03642806\",\n",
        "    \"cellphone\": \"n02992529\",\n",
        "    \"desk\": \"n03179701\",\n",
        "    \"station_wagon\": \"n02814533\",\n",
        "    \"pickup_truck\": \"n03930630\",\n",
        "    \"trailer_truck\": \"n04467665\"\n",
        "}\n",
        "synsets = {**tree_synsets, **nontree_synsets}\n",
        "print (\"Device:\" , _DEVICE)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device: cuda:0\n"
          ],
          "name": "stdout"
        }
>>>>>>> 26039cbb9be4bed73ea4ced92991de85923ab12d
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
<<<<<<< HEAD
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "mobilenet = models.mobilenet_v2(pretrained=True)\n",
    "# Preprocessing required for MobileNet v2\n",
    "\n",
    "    \n",
    "def expand(self, pil_img, boxes, target, background_color):\n",
    "    size = {}\n",
    "    size[\"width\"], size[\"height\"] = pil_img.size\n",
    "    if max(size) == \"width\":\n",
    "        size[\"new_height\"] = int(target/size[\"width\"] * size[\"height\"])\n",
    "        size[\"new_width\"] = int(target)\n",
    "    else:\n",
    "        size[\"new_width\"] = int(target/size[\"height\"] * size[\"width\"])\n",
    "        size[\"new_height\"] = int(target)\n",
    "\n",
    "\n",
    "    x_scale = size[\"new_width\"]/size[\"width\"]\n",
    "    y_scale = size[\"new_height\"]/size[\"height\"]\n",
    "\n",
    "    pil_img = pil_img.resize((size[\"new_width\"], size[\"new_height\"]))\n",
    "\n",
    "    xmin = boxes[0]\n",
    "    ymin = boxes[1]\n",
    "    xmax = boxes[2]\n",
    "    ymax = boxes[3]\n",
    "\n",
    "    xmin = int(np.round(xmin*x_scale)) + (target - size[\"new_width\"])/2\n",
    "    ymin = int(np.round(ymin*y_scale)) + (target - size[\"new_height\"])/2\n",
    "    xmax= int(np.round(xmax*(x_scale))) + (target - size[\"new_width\"])/2\n",
    "    ymax= int(np.round(ymax*y_scale)) + (target - size[\"new_height\"])/2\n",
    "\n",
    "    result = Image.new(pil_img.mode, (target, target), background_color)\n",
    "    result.paste(pil_img, (0, (size[\"new_width\"] - size[\"new_height\"]) // 2))\n",
    "\n",
    "    return result, torch.as_tensor([xmin, ymin, xmax, ymax], dtype=torch.float32)\n",
    "\n",
    "mobilenet_preprocessing = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n1JpU4M_iEY9"
   },
   "source": [
    "### Instantiate datasets, define loader processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_Y6XzYGG3ETT"
   },
   "outputs": [],
   "source": [
    "path = \"/content/drive/My Drive/data/imnet\"\n",
    "test_path = \"/content/drive/My Drive/data/test_greenstand_samples\"\n",
    "model_path = \"/content/drive/My Drive/models/ImageNet/mobilenet/%s\"%datetime.datetime.today().date()\n",
    "if not os.path.exists(model_path):\n",
    "  os.makedirs(model_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
=======
      "cell_type": "markdown",
      "metadata": {
        "id": "Wshbg0pg3IHX"
      },
      "source": [
        "\n",
        "## Dataset Creation\n",
        "Define datasets for ImageNet and Greenstand sources. Greenstand species classes are yet unlabeled."
      ]
>>>>>>> 26039cbb9be4bed73ea4ced92991de85923ab12d
    },
    "colab_type": "code",
    "id": "kqgQQtXy3b0q",
    "outputId": "608cc2a1-a3df-419a-d753-0115c60251b4"
   },
   "outputs": [
    {
<<<<<<< HEAD
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished creating datasets in  1084.453230381012  seconds \n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "data_set = ImnetDataset(path, synsets, transforms=mobilenet_preprocessing, one_hot=False, device=_DEVICE)\n",
    "greenstand_test = GreenstandDataset(test_path, transforms=mobilenet_preprocessing, one_hot=False, device=_DEVICE)\n",
    "\n",
    "print (\"Finished creating datasets in \", time.time() - start, \" seconds \") # this can take ~30 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kk6-8FA8Ab8v"
   },
   "outputs": [],
   "source": [
    "# Helper functions \n",
    "def rmse(x, y):\n",
    "  '''\n",
    "  Root-mean squared error of two vectors of the same batch \n",
    "  '''\n",
    "  return torch.sqrt( (1/x.size()[0]) * torch.sum((x-y) **2))\n",
    "\n",
    "def iou(box_a, box_b):\n",
    "  # order is xmin, ymin, xmax, ymax \n",
    "  intersect_xmin = max(box_a[0], box_b[0])\n",
    "  intersect_ymin = max(box_a[1], box_b[1])\n",
    "  intersect_xmax = min(box_a[2], box_b[2])\n",
    "  intersect_ymax = min(box_a[3], box_b[3])\n",
    "  area_intersect = max(0, intersect_xmax - intersect_xmin) * max(0, intersect_ymax - intersect_ymin)\n",
    "\n",
    "  area_a = (box_a[3] - box_a[1]) * (box_a[2] - box_a[0])\n",
    "  area_b = (box_b[3] - box_b[1]) * (box_b[2] - box_b[0])\n",
    "  union = area_a + area_b - area_intersect\n",
    "  return area_intersect / union\n",
    "\n",
    "class Customized_MobileNet(nn.Module):\n",
    "  def __init__(self, pretrained_model, num_classes=10):\n",
    "    super().__init__()\n",
    "    self.pretrained = pretrained_model\n",
    "    for param in self.pretrained.parameters():\n",
    "      param.requires_grad = False\n",
    "    self.pretrained.classifier = nn.Identity()\n",
    "    self._classifier_layer(num_classes)\n",
    "    self._regressor_layer()\n",
    "\n",
    "    \n",
    "  def forward(self, x):\n",
    "    \"\"\"\n",
    "    The model is performing a regression on bounding boxes and a classifier \n",
    "    \"\"\"\n",
    "    return (self.classifier(self.pretrained(x)), self.regressor(self.pretrained(x)))\n",
    "\n",
    "  def _classifier_layer(self, num_classes):\n",
    "    \"\"\"\n",
    "    Initializes final classification layer for labeling genus, species, etc.\n",
    "    \"\"\"\n",
    "    self.classifier = nn.Sequential(\n",
    "                        nn.Dropout(0.2),\n",
    "                        nn.Linear(1280, num_classes) # 1280 is num_outputs of the last feature layer\n",
    "                      ) \n",
    "    for param in self.classifier.parameters():\n",
    "      param.requires_grad=True\n",
    "\n",
    "  def _regressor_layer(self):\n",
    "    \"\"\"\n",
    "    A bounding box output layer for predicting object location\n",
    "    This is currently designed to output exactly one bounding box\n",
    "    \"\"\"\n",
    "    self.regressor =  nn.Sequential(\n",
    "                        nn.Dropout(0.2), \n",
    "                        nn.Linear(1280, 4) # 1280 is num_outputs of the last feature layer\n",
    "                      )\n",
    "    for param in self.regressor.parameters():\n",
    "      param.requires_grad=True\n",
    "\n",
    "class ModelTrainer():\n",
    "  '''\n",
    "  An abstraction to help keep track of model parameters and run training. \n",
    "  '''\n",
    "  def __init__ (self, model, dataset, learning_rate, alpha, beta, device, batch_size, model_savepath,\n",
    "                gamma=1e-4, train_split=0.8, pin_memory=False, n_workers=0):\n",
    "    \n",
    "    self.model = model # like Customized_MobileNet\n",
    "    self.model_savepath = os.path.join (model_savepath, \"checkpoint.pth.tar\")\n",
    "    # Initialize device\n",
    "    self.device = device\n",
    "    if self.device == torch.device(\"cuda:0\"):\n",
    "      self.model.cuda()\n",
    "\n",
    "    # Make validation split\n",
    "    self.trainsize = int(train_split * len(dataset))\n",
    "    self.valsize = len(dataset) - self.trainsize\n",
    "    train_dataset, valid_dataset = torch.utils.data.dataset.random_split(dataset, [self.trainsize, self.valsize])\n",
    "\n",
    "    # Define data loader for training and validation\n",
    "    self.batch_size = batch_size\n",
    "    self.data_loader  = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True, sampler=None,\n",
    "              batch_sampler=None, num_workers=n_workers, collate_fn=None,\n",
    "              pin_memory=pin_memory, drop_last=False, timeout=0,\n",
    "              worker_init_fn=None)\n",
    "\n",
    "    self.val_data_loader = DataLoader(valid_dataset, batch_size=self.batch_size, shuffle=True, sampler=None,\n",
    "              batch_sampler=None, num_workers=n_workers, collate_fn=None,\n",
    "              pin_memory=pin_memory, drop_last=False, timeout=0,\n",
    "              worker_init_fn=None)\n",
    "    \n",
    "    # Loss specifications and optimizer parameter setting\n",
    "    # This is defined here so that the underlying model can be changed (i.e. hidden layers) \n",
    "    self.alpha, self.beta = alpha, beta # classification/regression loss tradeoff\n",
    "    cps = [param for param in self.model.classifier.parameters()]\n",
    "    rps = [param for param in self.model.regressor.parameters()]\n",
    "    self.optimizer = torch.optim.Adam(params=cps+rps, lr=learning_rate, weight_decay=gamma)\n",
    "    self.regression_criterion = nn.MSELoss()\n",
    "    self.classification_criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    if os.path.exists(self.model_savepath):\n",
    "      print (\"Found saved model at savepath %s\" %(self.model_savepath))\n",
    "      checkpoint = torch.load(self.model_savepath)\n",
    "      self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "      self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "      self.start_epoch = checkpoint['epoch']\n",
    "    else:\n",
    "      self.start_epoch = 0\n",
    "\n",
    "\n",
    "  def describe_training(self):\n",
    "    print (self.trainsize, \" training examples\")\n",
    "    print (self.valsize, \" validation examples\")\n",
    "\n",
    "  def train(self, num_epochs, val_interval=1, batch_report=50, batch_lookback=10):\n",
    "    '''\n",
    "    Main function to train. \n",
    "    @param num_epochs(int): Number of epochs of training\n",
    "    @param val_interval(int): Interval epochs between validation metric\n",
    "    @param batch_report(int): Interval batches between training reports\n",
    "    @param batch_lookback(int): Number of batches to use for averaging metrics in printing\n",
    "    '''\n",
    "    num_tr_batches = np.ceil(self.trainsize/self.data_loader.batch_size)\n",
    "    num_val_batches = np.ceil(self.valsize/self.valsize)\n",
    "    epoch_loss = []\n",
    "    epoch_acc = []\n",
    "    epoch_rmse = []\n",
    "    epoch_iou = []\n",
    "    val_epoch_loss = []\n",
    "    val_epoch_acc = []\n",
    "    val_epoch_rmse = []\n",
    "    val_epoch_iou = []\n",
    "    print (\"Starting at epoch %d\"%self.start_epoch)\n",
    "    for epoch in range(self.start_epoch, num_epochs):\n",
    "      epoch_start = time.time()\n",
    "      print (\"=\" * 50)\n",
    "      print (\"EPOCH \", epoch)\n",
    "      batch_count = 0\n",
    "      batch_loss = []\n",
    "      batch_acc = []\n",
    "      batch_rmse = []\n",
    "      batch_iou = []\n",
    "      for batchx, batchy in self.data_loader:\n",
    "          batch_count += 1\n",
    "          # Device designation\n",
    "          if self.device == torch.device(\"cuda:0\"):\n",
    "            batchx = batchx.cuda(non_blocking=True)\n",
    "            batchy[\"boxes\"] = batchy[\"boxes\"].cuda(non_blocking=True)\n",
    "            batchy[\"image_class\"] = batchy[\"image_class\"].cuda(non_blocking=True)\n",
    "          \n",
    "          class_labels = batchy[\"image_class\"]\n",
    "          box_labels = batchy[\"boxes\"]\n",
    "\n",
    "          # Forward pass\n",
    "          class_preds, box_preds = self.model.forward(batchx)\n",
    "          loss = self._loss_specification(class_preds, class_labels, box_preds, box_labels)\n",
    "          loss.backward()\n",
    "          self.optimizer.step()\n",
    "\n",
    "          # Metrics \n",
    "          box_rmse = rmse(box_preds, box_labels)\n",
    "          avg_box_iou = torch.mean(torch.as_tensor([iou(box_labels[i, :], box_preds[i, :]) for i in range(box_labels.size()[0])], dtype=torch.float32))\n",
    "          class_correct = (torch.max(class_preds, 1)[1] == class_labels.squeeze()).sum()\n",
    "          acc = class_correct/float(batchx.shape[0])\n",
    "          batch_iou.append(avg_box_iou)\n",
    "          batch_rmse.append(box_rmse)\n",
    "          batch_acc.append(acc)\n",
    "          batch_loss.append(loss.data)\n",
    "\n",
    "          if batch_count % batch_report == 0 or batch_count == num_tr_batches:\n",
    "            print (\"\\nLast %d Batch Avg Metrics, Batch %d/%d\" %(batch_lookback, batch_count, num_tr_batches))\n",
    "            print (\"Total Loss: {:.3f}\".format(torch.mean(torch.as_tensor(batch_loss[-batch_lookback:], dtype=torch.float32))))\n",
    "            print (\"Classification Acc: {:.3f}\".format(torch.mean(torch.as_tensor(batch_acc[-batch_lookback:], dtype=torch.float32))))\n",
    "            print (\"BBox RMSE: {:.3f}\".format(torch.mean(torch.as_tensor(batch_rmse[-batch_lookback:], dtype=torch.float32))))\n",
    "            print (\"Avg Bbox IoU: {:.3f} \\n\".format(torch.mean(torch.as_tensor(batch_iou[-batch_lookback:], dtype=torch.float32))))\n",
    "            torch.save({\n",
    "                          'epoch': epoch + 1,\n",
    "                          'model_state_dict': self.model.state_dict(),\n",
    "                          'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "                          }, \n",
    "                       self.model_savepath)\n",
    "            print (\"Checkpoint created\")\n",
    "      \n",
    "      if epoch % val_interval == 0:\n",
    "          print (\"VALIDATION EPOCH \", epoch)\n",
    "          batch_count = 0\n",
    "          \n",
    "          self.model.eval()\n",
    "          with torch.no_grad():\n",
    "            rmses = []\n",
    "            ious = []\n",
    "            losses = []\n",
    "            class_accs = []\n",
    "\n",
    "            for batchx, batchy in self.val_data_loader:\n",
    "                batch_count += 1\n",
    "                 # Device designation\n",
    "                if self.device == torch.device(\"cuda:0\"):\n",
    "                  batchx = batchx.cuda(non_blocking=True)\n",
    "                  batchy[\"boxes\"] = batchy[\"boxes\"].cuda(non_blocking=True)\n",
    "                  batchy[\"image_class\"] = batchy[\"image_class\"].cuda(non_blocking=True)\n",
    "                \n",
    "                class_labels = batchy[\"image_class\"]\n",
    "                box_labels = batchy[\"boxes\"]\n",
    "                class_preds, box_preds = self.model.forward(batchx)\n",
    "                losses.append(self._loss_specification(class_preds, class_labels, box_preds, box_labels).data)\n",
    "                class_accs.append(float((torch.max(class_preds, 1)[1] == class_labels.squeeze()).sum())/self.val_data_loader.batch_size)\n",
    "                ious.append(torch.mean(torch.as_tensor([iou(box_labels[i, :], box_preds[i, :]) for i in range(box_labels.size()[0])], dtype=torch.float32)))\n",
    "                rmses.append(rmse(box_preds, box_labels))\n",
    "\n",
    "              \n",
    "            losses = torch.mean(torch.as_tensor(losses, dtype=torch.float32))\n",
    "            class_accs = torch.mean(torch.as_tensor(class_accs, dtype=torch.float32))\n",
    "            box_rmse = torch.mean(torch.as_tensor(rmses, dtype=torch.float32))\n",
    "            avg_box_iou = torch.mean(torch.as_tensor(ious, dtype=torch.float32))\n",
    "            val_epoch_loss.append(losses)\n",
    "            val_epoch_acc.append(class_accs)\n",
    "            val_epoch_rmse.append(box_rmse)\n",
    "            val_epoch_iou.append(avg_box_iou)\n",
    "\n",
    "            # We can change this to be epoch wise or not averaged over all batches\n",
    "            print (\"Batch Average Val Loss: {:.3f}\".format(losses))\n",
    "            print (\"Batch Avg Val Classification Acc: {:.3f}\".format(class_accs))\n",
    "            print (\"Batch Avg Val BBox RMSE: {:.3f}\".format(box_rmse))\n",
    "            print (\"Batch Avg Avg Bbox IoU: {:.3f} \\n\".format(avg_box_iou))\n",
    "          self.model.train()\n",
    "      epoch_loss.append(torch.mean(torch.as_tensor(batch_loss, dtype=torch.float32)))\n",
    "      epoch_acc.append(torch.mean(torch.as_tensor(batch_acc, dtype=torch.float32)))\n",
    "      epoch_iou.append(torch.mean(torch.as_tensor(batch_iou, dtype=torch.float32)))\n",
    "      epoch_rmse.append(torch.mean(torch.as_tensor(batch_rmse, dtype=torch.float32)))\n",
    "  \n",
    "      print (\"Epoch \", epoch + 1, \" finished in \", time.time() - epoch_start)\n",
    "    tr_metric_dict = {\"Loss\": epoch_loss, \"Acc\": epoch_acc, \"IoU\": epoch_iou, \"RMSE\": epoch_rmse}\n",
    "    val_metric_dict = {\"Loss\": val_epoch_loss, \"Acc\": val_epoch_acc, \"IoU\": val_epoch_iou, \"RMSE\": val_epoch_rmse}\n",
    "    torch.save({\n",
    "              'epoch': epoch + 1,\n",
    "              'model_state_dict': self.model.state_dict(),\n",
    "              'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "              'tr_metric_dict': tr_metric_dict, \n",
    "              'val_metric_dict': val_metric_dict\n",
    "              }, \n",
    "            self.model_savepath)\n",
    "    print (\"Final checkpoint created. Model dict and metrics saved. \")\n",
    "    return self.model, tr_metric_dict, val_metric_dict\n",
    "\n",
    "\n",
    "  def _loss_specification(self, class_preds, class_labels, box_preds, box_labels):\n",
    "    bounding_box_error = self.regression_criterion(box_preds, box_labels) # bounding box regression error\n",
    "    # preds_flat = torch.max(class_preds, dim=1)[1] # BSIZE x 1 (index)\n",
    "    classification_loss = self.classification_criterion(class_preds, class_labels.view(-1)) # classification error\n",
    "    return self.alpha * classification_loss + self.beta * bounding_box_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
=======
      "cell_type": "code",
      "metadata": {
        "id": "LJ2MNziQ3B-B"
      },
      "source": [
        "class ImnetDataset(data.Dataset):\n",
        "    \n",
        "    # initialise function of class\n",
        "    def __init__(self, dir, synsets, transforms=None, device=None, one_hot=False, nontrees=False):\n",
        "        # the data directories\n",
        "        self.img_dir = os.path.join(dir, \"original_images\")\n",
        "        self.bb_dir = os.path.join(dir, \"bounding_boxes\")\n",
        "        self.nontrees_present = nontrees\n",
        "        #synsets library to get the associated class\n",
        "        if not self.nontrees_present:  # only tree images\n",
        "          self.synsets = tree_synsets\n",
        "        else: # mix other things\n",
        "          self.synsets = synsets\n",
        "        self.rev_synsets = {y:x for x,y in zip(synsets.keys(), synsets.values())}\n",
        "        self.classes = list(self.synsets.keys())\n",
        "\n",
        "        self.one_hot = one_hot\n",
        "        self.imgs = []\n",
        "\n",
        "        for i in self.classes:\n",
        "          temp_imgs = list(sorted(os.listdir(os.path.join(self.img_dir, i))))\n",
        "          for img_path in temp_imgs:\n",
        "            #in every directory the \"tar\" file is still present\n",
        "            if not \"tar\" in img_path:\n",
        "              name = img_path.split('.')[0]\n",
        "              self.imgs.append(name)\n",
        "\n",
        "        self.bb_dict = {}\n",
        "        for f, _, d in os.walk(self.bb_dir):\n",
        "          for file in d:\n",
        "            if os.path.splitext(file)[1] == \".xml\" and file.split(\"_\")[0] in tree_synsets.values():\n",
        "              tree = ElementTree.parse(os.path.join(f, file))\n",
        "              root = tree.getroot()\n",
        "              obj = root.find(\"object\")\n",
        "              b = obj.find(\"bndbox\")\n",
        "              xmin = int(b.find(\"xmin\").text)\n",
        "              ymin = int(b.find(\"ymin\").text)\n",
        "              xmax = int(b.find(\"xmax\").text)\n",
        "              ymax = int(b.find(\"ymax\").text)\n",
        "              self.bb_dict[os.path.join(f, file)] =  (xmin, ymin, xmax, ymax)\n",
        "\n",
        "        self.transforms = transforms\n",
        "        self.device = device\n",
        "        \n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        name = self.imgs[idx]\n",
        "        label = self.rev_synsets[name.split(\"_\")[0]]\n",
        "        # modify filters to determine if trees present\n",
        "        is_tree = 1.0\n",
        "        if self.nontrees_present:\n",
        "          if label in tree_synsets.keys():\n",
        "            is_tree = 1.0\n",
        "          else:\n",
        "            is_tree = 0.0\n",
        "\n",
        "        img_path = os.path.join(self.img_dir, label, f\"{name}.JPEG\")\n",
        "        bb_path = os.path.join(self.bb_dir, label, \"Annotation\", name.split(\"_\")[0], f\"{name}.xml\")\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "\n",
        "        if bb_path in self.bb_dict.keys():\n",
        "          xmin, ymin, xmax, ymax = self.bb_dict[bb_path]\n",
        "        else:\n",
        "          # the whole image is the bounding box label, as NoneType was causing collating issue. \n",
        "          xmin = 0\n",
        "          ymin = 0\n",
        "          xmax = img.size[0]\n",
        "          ymax = img.size[1]\n",
        "        boxes = torch.as_tensor([xmin, ymin, xmax, ymax], dtype=torch.float32)\n",
        "        if not is_tree:\n",
        "          boxes = torch.as_tensor([0, 0, 0, 0], dtype=torch.float32)  # 0 out nontree bounding boxes, don't want predictions for these\n",
        "        \n",
        "        if self.transforms is not None:\n",
        "          img = self.transforms(img)\n",
        "\n",
        "        if self.one_hot: \n",
        "          image_id = torch.zeros(len(self.classes), dtype=torch.float32)\n",
        "          image_id [self.classes.index(label)] = 1.0\n",
        "        else:\n",
        "          image_id = torch.tensor([self.classes.index(label)])\n",
        "\n",
        "        targets = {}\n",
        "        targets[\"boxes\"] = boxes\n",
        "        targets[\"image_class\"] = image_id\n",
        "        targets[\"is_tree\"] = is_tree\n",
        "    \n",
        "        return img, targets\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs)\n"
      ],
      "execution_count": null,
      "outputs": []
>>>>>>> 26039cbb9be4bed73ea4ced92991de85923ab12d
    },
    "colab_type": "code",
    "id": "U6TrQ9DDL6HD",
    "outputId": "319fe3b4-fe48-4456-9c66-5b1a4718892a"
   },
   "outputs": [
    {
<<<<<<< HEAD
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23085  training examples\n",
      "5772  validation examples\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "torch.multiprocessing.set_start_method(\"spawn\", force=True)\n",
    "\n",
    "_N_WORKERS = 0\n",
    "_PIN_MEM = True\n",
    "\n",
    "mob_model = Customized_MobileNet(pretrained_model=mobilenet, num_classes=28)\n",
    "trainer = ModelTrainer(mob_model, \n",
    "                       data_set, \n",
    "                       train_split=0.8,\n",
    "                       learning_rate=0.002,\n",
    "                       batch_size = 32,\n",
    "                       alpha=0.2, # classification loss weight \n",
    "                       beta=0.8, # regression loss weight\n",
    "                       device=_DEVICE, \n",
    "                       pin_memory=_PIN_MEM,\n",
    "                       n_workers=_N_WORKERS,\n",
    "                       model_savepath=model_path)\n",
    "print (trainer.describe_training())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xl5ULX4vu4YB"
   },
   "source": [
    "### Check model structure and parameters\n",
    "Regressor and classifier (final layer) should require grad. Others should not. Optimizer should be set only to those regressor and classifier variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 534
=======
      "cell_type": "code",
      "metadata": {
        "id": "QOicVbPgZrK1"
      },
      "source": [
        "class GreenstandDataset(data.Dataset):\n",
        "  # We don't have labels for this yet...\n",
        "  def __init__(self, dir, device, transforms, bb_dir=None, one_hot=False):\n",
        "    self.img_dir = dir\n",
        "    self.imgs = []\n",
        "    self.bb_dir = bb_dir\n",
        "    self.transforms = transforms\n",
        "    self.classes = None # Change this when we define Greenstand class labels\n",
        "    self.one_hot = one_hot\n",
        "    self.device = device\n",
        "\n",
        "    for f, _, d in os.walk(test_path):\n",
        "      for fil in d:\n",
        "        if \"jpg\" in fil:\n",
        "          self.imgs.append(os.path.join(f, fil))\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    img = Image.open(self.img_dir[idx])\n",
        "    if self.transforms is not None:\n",
        "      img = self.transforms (img)\n",
        "\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.imgs)"
      ],
      "execution_count": null,
      "outputs": []
>>>>>>> 26039cbb9be4bed73ea4ced92991de85923ab12d
    },
    "colab_type": "code",
    "id": "SdO1y3kTtfpM",
    "outputId": "353dc64a-01d1-4c4c-c5d2-c73e7706bec0"
   },
   "outputs": [
    {
<<<<<<< HEAD
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regressor params\n",
      "[Parameter containing:\n",
      "tensor([[ 0.0035, -0.0260,  0.0100,  ...,  0.0091,  0.0158,  0.0058],\n",
      "        [-0.0107,  0.0251,  0.0188,  ..., -0.0173, -0.0017,  0.0056],\n",
      "        [ 0.0277,  0.0120, -0.0125,  ..., -0.0255,  0.0147,  0.0213],\n",
      "        [-0.0270,  0.0020, -0.0223,  ...,  0.0029,  0.0227,  0.0134]],\n",
      "       device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0054, -0.0066,  0.0226,  0.0130], device='cuda:0',\n",
      "       requires_grad=True)]\n",
      "Classifier params\n",
      "[Parameter containing:\n",
      "tensor([[-0.0036, -0.0161, -0.0080,  ..., -0.0157,  0.0006, -0.0255],\n",
      "        [ 0.0234, -0.0214,  0.0207,  ...,  0.0203, -0.0175,  0.0183],\n",
      "        [-0.0215,  0.0207,  0.0056,  ..., -0.0185, -0.0004, -0.0115],\n",
      "        ...,\n",
      "        [ 0.0113,  0.0216,  0.0130,  ..., -0.0172, -0.0082, -0.0274],\n",
      "        [-0.0236,  0.0036, -0.0254,  ..., -0.0222, -0.0032,  0.0175],\n",
      "        [ 0.0041,  0.0249,  0.0242,  ...,  0.0209,  0.0066, -0.0240]],\n",
      "       device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0100, -0.0270,  0.0038, -0.0181, -0.0139, -0.0138,  0.0262,  0.0223,\n",
      "        -0.0239,  0.0002, -0.0266, -0.0050,  0.0093, -0.0239, -0.0253, -0.0081,\n",
      "        -0.0018,  0.0127,  0.0191,  0.0255, -0.0058, -0.0055,  0.0108, -0.0258,\n",
      "         0.0215, -0.0021,  0.0096, -0.0165], device='cuda:0',\n",
      "       requires_grad=True)]\n",
      "torch.Size([28, 1280])\n",
      "torch.Size([28])\n",
      "torch.Size([4, 1280])\n",
      "torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "print (\"Regressor params\")\n",
    "print ([p for p in mob_model.regressor.parameters()])\n",
    "\n",
    "print (\"Classifier params\")\n",
    "print ([p for p in mob_model.classifier.parameters()])\n",
    "\n",
    "# Should output just two layers (4 variables total)\n",
    "for param in mob_model.parameters():\n",
    "  if param.requires_grad:\n",
    "    print (param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
=======
      "cell_type": "markdown",
      "metadata": {
        "id": "bAGG9gAcEgdv"
      },
      "source": [
        "## MobileNet-v2\n",
        "See [the original paper](https://arxiv.org/pdf/1704.04861.pdf) for details. This was chosen first because Torchvision has pretrained weights and the net is quite low-latency, which may be useful for user-interface image selection. First go is to simply change the output layer to predict 4 coordinates for the bounding box."
      ]
>>>>>>> 26039cbb9be4bed73ea4ced92991de85923ab12d
    },
    "colab_type": "code",
    "id": "72xHKgN4RLah",
    "outputId": "ec4b990e-d2ff-44f5-d602-e11dd3e0eeaf"
   },
   "outputs": [
    {
<<<<<<< HEAD
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting at epoch 0\n",
      "==================================================\n",
      "EPOCH  0\n",
      "\n",
      "Last 10 Batch Avg Metrics, Batch 50/722\n",
      "Total Loss: 58400.125\n",
      "Classification Acc: 0.141\n",
      "BBox RMSE: 536.654\n",
      "Avg Bbox IoU: 0.036 \n",
      "\n",
      "Checkpoint created\n",
      "\n",
      "Last 10 Batch Avg Metrics, Batch 100/722\n",
      "Total Loss: 59374.426\n",
      "Classification Acc: 0.112\n",
      "BBox RMSE: 539.279\n",
      "Avg Bbox IoU: 0.160 \n",
      "\n",
      "Checkpoint created\n",
      "\n",
      "Last 10 Batch Avg Metrics, Batch 150/722\n",
      "Total Loss: 26664.906\n",
      "Classification Acc: 0.116\n",
      "BBox RMSE: 356.483\n",
      "Avg Bbox IoU: 0.344 \n",
      "\n",
      "Checkpoint created\n",
      "\n",
      "Last 10 Batch Avg Metrics, Batch 200/722\n",
      "Total Loss: 34268.129\n",
      "Classification Acc: 0.119\n",
      "BBox RMSE: 392.937\n",
      "Avg Bbox IoU: 0.379 \n",
      "\n",
      "Checkpoint created\n",
      "\n",
      "Last 10 Batch Avg Metrics, Batch 250/722\n",
      "Total Loss: 48418.023\n",
      "Classification Acc: 0.106\n",
      "BBox RMSE: 458.910\n",
      "Avg Bbox IoU: 0.341 \n",
      "\n",
      "Checkpoint created\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/PIL/TiffImagePlugin.py:788: UserWarning: Corrupt EXIF data.  Expecting to read 2 bytes but only got 0. \n",
      "  warnings.warn(str(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Last 10 Batch Avg Metrics, Batch 300/722\n",
      "Total Loss: 38705.789\n",
      "Classification Acc: 0.181\n",
      "BBox RMSE: 428.677\n",
      "Avg Bbox IoU: 0.351 \n",
      "\n",
      "Checkpoint created\n"
     ]
    }
   ],
   "source": [
    "trainer.train(num_epochs=10,batch_report=50) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xBJ_wqhDIkoF"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "tree_detection.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "environment": {
   "name": "pytorch-gpu.1-4.m55",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-4:m55"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "21a488cfd67f40a08af10fabf4ca8e48": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_85e4de55e35e42b0a3ebcb2b7a8f5411",
       "IPY_MODEL_79cd02a9290d4913a590c1812da0b714"
=======
      "cell_type": "code",
      "metadata": {
        "id": "5g48vTx64Wdz",
        "outputId": "d2b7daf8-b973-4b11-bc5a-4fa33b8a7b64",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86,
          "referenced_widgets": [
            "9e1f88b442454b538d64c9cff7505411",
            "fb3c6a1d4edc4c78b1b2ad823838bbf1",
            "e7bee5115bcc4142b743981a0011bdf4",
            "05125a8cfbb1478ea40a0772830b4677",
            "d86ed8e90eda4e4a90d429232fc9e62a",
            "d217d5c345d840c58404121e1f5257e9",
            "04f548f0be6b461e9252fb102896eea0",
            "f62c4ea6649a45bf88564acba82add1a"
          ]
        }
      },
      "source": [
        "mobilenet = models.mobilenet_v2(pretrained=True)\n",
        "# Preprocessing required for MobileNet v2\n",
        "mobilenet_preprocessing = transforms.Compose([\n",
        "    transforms.Resize((128, 128)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/mobilenet_v2-b0353104.pth\" to /root/.cache/torch/hub/checkpoints/mobilenet_v2-b0353104.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9e1f88b442454b538d64c9cff7505411",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=14212972.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1JpU4M_iEY9"
      },
      "source": [
        "### Instantiate datasets, define loader processes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Y6XzYGG3ETT"
      },
      "source": [
        "path = \"/content/drive/My Drive/data/imnet\"\n",
        "test_path = \"/content/drive/My Drive/data/test_greenstand_samples\"\n",
        "model_path = \"/content/drive/My Drive/models/ImageNet/detection/mobilenet/%s\"%datetime.datetime.today().date()\n",
        "if not os.path.exists(model_path):\n",
        "  os.makedirs(model_path)\n",
        "\n"
>>>>>>> 26039cbb9be4bed73ea4ced92991de85923ab12d
      ],
      "layout": "IPY_MODEL_9b87637d692043f888a16d0912f7c900"
     }
    },
<<<<<<< HEAD
    "37dd780c4fba4d66a78d06ee8b1183de": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "70755204654046b997b9d312154ef497": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "79cd02a9290d4913a590c1812da0b714": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_70755204654046b997b9d312154ef497",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_37dd780c4fba4d66a78d06ee8b1183de",
      "value": " 13.6M/13.6M [00:00&lt;00:00, 41.3MB/s]"
     }
    },
    "85e4de55e35e42b0a3ebcb2b7a8f5411": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f2251329df3b41db8c6e5907a4c02d05",
      "max": 14212972,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d4e996e48eac4bb191412a29a46388e3",
      "value": 14212972
     }
    },
    "9b87637d692043f888a16d0912f7c900": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d4e996e48eac4bb191412a29a46388e3": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "f2251329df3b41db8c6e5907a4c02d05": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
=======
    {
      "cell_type": "code",
      "metadata": {
        "id": "kqgQQtXy3b0q",
        "outputId": "b0c080d1-8c02-4237-d9e4-67983ad34ec0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "start = time.time()\n",
        "data_set = ImnetDataset(path, synsets, transforms=mobilenet_preprocessing, one_hot=False, device=_DEVICE, nontrees=True)\n",
        "greenstand_test = GreenstandDataset(test_path, transforms=mobilenet_preprocessing, one_hot=False, device=_DEVICE)\n",
        "\n",
        "print (\"Finished creating datasets in \", time.time() - start, \" seconds \") # this can take ~60 minutes"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Finished creating datasets in  2035.394990682602  seconds \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kk6-8FA8Ab8v"
      },
      "source": [
        "\n",
        "# Helper functions \n",
        "def rmse(x, y):\n",
        "  '''\n",
        "  Root-mean squared error of two vectors of the same batch \n",
        "  '''\n",
        "  return torch.sqrt( (1/x.size()[0]) * torch.sum((x-y) **2))\n",
        "\n",
        "def iou(box_a, box_b):\n",
        "  # order is xmin, ymin, xmax, ymax \n",
        "  intersect_xmin = max(box_a[0], box_b[0])\n",
        "  intersect_ymin = max(box_a[1], box_b[1])\n",
        "  intersect_xmax = min(box_a[2], box_b[2])\n",
        "  intersect_ymax = min(box_a[3], box_b[3])\n",
        "  area_intersect = max(0, intersect_xmax - intersect_xmin) * max(0, intersect_ymax - intersect_ymin)\n",
        "\n",
        "  area_a = (box_a[3] - box_a[1]) * (box_a[2] - box_a[0])\n",
        "  area_b = (box_b[3] - box_b[1]) * (box_b[2] - box_b[0])\n",
        "  union = area_a + area_b - area_intersect\n",
        "  return area_intersect / union\n",
        "\n",
        "class Customized_MobileNet(nn.Module):\n",
        "  def __init__(self, pretrained_model):\n",
        "    super().__init__()\n",
        "    self.pretrained = pretrained_model\n",
        "    self.pretrained.classifier = nn.Identity()\n",
        "    for param in self.pretrained.parameters():\n",
        "      param.requires_grad = False\n",
        "    self._binary_classifier_layer()\n",
        "    self._regressor_layer()\n",
        "\n",
        "    \n",
        "  def forward(self, x):\n",
        "    \"\"\"\n",
        "    The model is performing a regression on bounding boxes and a classifier \n",
        "    \"\"\"\n",
        "    return self.classifier(self.pretrained(x)), self.regressor(self.pretrained(x))\n",
        "\n",
        "  def _binary_classifier_layer(self):\n",
        "    \"\"\"\n",
        "    Initializes final classification layer for labeling genus, species, etc.\n",
        "    \"\"\"\n",
        "    self.classifier = nn.Sequential(\n",
        "                        nn.Dropout(0.2),\n",
        "                        nn.Linear(1280, 1), # 1280 is num_outputs of the last feature layer\n",
        "                      ) \n",
        "    for param in self.classifier.parameters():\n",
        "      param.requires_grad = True\n",
        "\n",
        "  def _regressor_layer(self):\n",
        "    \"\"\"\n",
        "    A bounding box output layer for predicting object location\n",
        "    This is currently designed to output exactly one bounding box\n",
        "    \"\"\"\n",
        "    self.regressor =  nn.Sequential(\n",
        "                        nn.Dropout(0.2), \n",
        "                        nn.Linear(1280, 4) # 1280 is num_outputs of the last feature layer\n",
        "                      )\n",
        "    for param in self.regressor.parameters():\n",
        "      param.requires_grad=True\n",
        "\n",
        "class ModelTrainer():\n",
        "  '''\n",
        "  An abstraction to help keep track of model parameters and run training. \n",
        "  '''\n",
        "  def __init__ (self, model, dataset, learning_rate, device, batch_size, model_savepath,\n",
        "                gamma=1e-4, train_split=0.8, pin_memory=False, n_workers=0,\n",
        "                alpha=0.5, beta=0.5\n",
        "                ):\n",
        "    \n",
        "    self.model = model # like Customized_MobileNet\n",
        "    self.model_savepath = os.path.join (model_savepath, \"checkpoint.pth.tar\")\n",
        "    self.alpha = alpha\n",
        "    self.beta = beta\n",
        "    # Initialize device\n",
        "    self.device = device\n",
        "    if self.device == torch.device(\"cuda:0\"):\n",
        "      self.model.cuda()\n",
        "    \n",
        "\n",
        "    # Make validation split\n",
        "    self.trainsize = int(train_split * len(dataset))\n",
        "    self.valsize = len(dataset) - self.trainsize\n",
        "    train_dataset, valid_dataset = torch.utils.data.dataset.random_split(dataset, [self.trainsize, self.valsize])\n",
        "\n",
        "    # Define data loader for training and validation\n",
        "    self.batch_size = batch_size\n",
        "    self.data_loader  = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True, sampler=None,\n",
        "              batch_sampler=None, num_workers=n_workers, collate_fn=None,\n",
        "              pin_memory=pin_memory, drop_last=False, timeout=0,\n",
        "              worker_init_fn=None)\n",
        "\n",
        "    self.val_data_loader = DataLoader(valid_dataset, batch_size=self.batch_size, shuffle=True, sampler=None,\n",
        "              batch_sampler=None, num_workers=n_workers, collate_fn=None,\n",
        "              pin_memory=pin_memory, drop_last=False, timeout=0,\n",
        "              worker_init_fn=None)\n",
        "    \n",
        "    # Loss specifications and optimizer parameter setting\n",
        "    # This is defined here so that the underlying model can be changed (i.e. hidden layers) \n",
        "    cps = [param for param in self.model.classifier.parameters()]\n",
        "    rps = [param for param in self.model.regressor.parameters()]\n",
        "    self.optimizer = torch.optim.Adam(params=cps+rps, lr=learning_rate, weight_decay=gamma)\n",
        "    self.binary_classification_criterion = nn.BCEWithLogitsLoss()\n",
        "    self.regression_criterion = nn.MSELoss()\n",
        "\n",
        "    if os.path.exists(self.model_savepath):\n",
        "      print (\"Found saved model at savepath %s\" %(self.model_savepath))\n",
        "      checkpoint = torch.load(self.model_savepath)\n",
        "      self.model.load_state_dict(checkpoint['model_state_dict'])\n",
        "      self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "      self.start_epoch = checkpoint['epoch']\n",
        "    else:\n",
        "      self.start_epoch = 0\n",
        "\n",
        "\n",
        "  def describe_training(self):\n",
        "    print (self.trainsize, \" training examples\")\n",
        "    print (self.valsize, \" validation examples\")\n",
        "\n",
        "  def train(self, num_epochs, val_interval=1, batch_report=50, batch_lookback=10):\n",
        "    '''\n",
        "    Main function to train. \n",
        "    @param num_epochs(int): Number of epochs of training\n",
        "    @param val_interval(int): Interval epochs between validation metric\n",
        "    @param batch_report(int): Interval batches between training reports\n",
        "    @param batch_lookback(int): Number of batches to use for averaging metrics in printing\n",
        "    '''\n",
        "    num_tr_batches = np.ceil(self.trainsize/self.data_loader.batch_size)\n",
        "    num_val_batches = np.ceil(self.valsize/self.valsize)\n",
        "    epoch_loss = []\n",
        "    epoch_acc = []\n",
        "    epoch_rmse = []\n",
        "    epoch_iou = []\n",
        "    val_epoch_loss = []\n",
        "    val_epoch_acc = []\n",
        "    val_epoch_rmse = []\n",
        "    val_epoch_iou = []\n",
        "    print (\"Starting at epoch %d\"%self.start_epoch)\n",
        "    for epoch in range(self.start_epoch, num_epochs):\n",
        "      epoch_start = time.time()\n",
        "      print (\"=\" * 50)\n",
        "      print (\"EPOCH \", epoch)\n",
        "      batch_count = 0\n",
        "      batch_loss = []\n",
        "      batch_acc = []\n",
        "      batch_rmse = []\n",
        "      batch_iou = []\n",
        "      for batchx, batchy in self.data_loader:\n",
        "          batch_count += 1\n",
        "          # Device designation\n",
        "          if self.device == torch.device(\"cuda:0\"):\n",
        "            batchx = batchx.cuda(non_blocking=True)\n",
        "            batchy[\"boxes\"] = batchy[\"boxes\"].cuda(non_blocking=True)\n",
        "            batchy[\"image_class\"] = batchy[\"image_class\"].cuda(non_blocking=True)\n",
        "            batchy[\"is_tree\"] = batchy[\"is_tree\"].cuda(non_blocking=True)\n",
        "          class_labels = batchy[\"image_class\"]\n",
        "          box_labels = batchy[\"boxes\"]\n",
        "          is_tree_labels = batchy[\"is_tree\"]\n",
        "\n",
        "          # Forward pass\n",
        "          is_tree_preds, box_preds = self.model.forward(batchx)\n",
        "          loss = self._loss_specification(is_tree_labels, is_tree_preds, box_labels, box_preds)\n",
        "          loss.backward()\n",
        "          self.optimizer.step()\n",
        "\n",
        "          # Metrics \n",
        "          box_rmse = rmse(box_preds, box_labels)\n",
        "          avg_box_iou = torch.mean(torch.as_tensor([iou(box_labels[i, :], box_preds[i, :]) for i in range(box_labels.size()[0])], dtype=torch.float32))\n",
        "          binary_correct = (torch.round(is_tree_preds) == is_tree_labels.squeeze()).sum()\n",
        "          acc = binary_correct/float(batchx.shape[0])\n",
        "          batch_iou.append(avg_box_iou)\n",
        "          batch_rmse.append(box_rmse)\n",
        "          batch_acc.append(acc)\n",
        "          batch_loss.append(loss.data)\n",
        "\n",
        "          if batch_count % batch_report == 0 or batch_count == num_tr_batches:\n",
        "            print (\"\\nLast %d Batch Avg Metrics, Batch %d/%d\" %(batch_lookback, batch_count, num_tr_batches))\n",
        "            print (\"Total Loss: {:.3f}\".format(torch.mean(torch.as_tensor(batch_loss[-batch_lookback:], dtype=torch.float32))))\n",
        "            print (\"Classification Acc: {:.3f}\".format(torch.mean(torch.as_tensor(batch_acc[-batch_lookback:], dtype=torch.float32))))\n",
        "            print (\"BBox RMSE: {:.3f}\".format(torch.mean(torch.as_tensor(batch_rmse[-batch_lookback:], dtype=torch.float32))))\n",
        "            print (\"Avg Bbox IoU: {:.3f} \\n\".format(torch.mean(torch.as_tensor(batch_iou[-batch_lookback:], dtype=torch.float32))))\n",
        "            torch.save({\n",
        "                          'epoch': epoch + 1,\n",
        "                          'model_state_dict': self.model.state_dict(),\n",
        "                          'optimizer_state_dict': self.optimizer.state_dict(),\n",
        "                          }, \n",
        "                       self.model_savepath)\n",
        "            print (\"Checkpoint created\")\n",
        "      \n",
        "      if epoch % val_interval == 0:\n",
        "          print (\"VALIDATION EPOCH \", epoch)\n",
        "          batch_count = 0\n",
        "          \n",
        "          self.model.eval()\n",
        "          with torch.no_grad():\n",
        "            rmses = []\n",
        "            ious = []\n",
        "            losses = []\n",
        "            class_accs = []\n",
        "\n",
        "            for batchx, batchy in self.val_data_loader:\n",
        "                batch_count += 1\n",
        "                 # Device designation\n",
        "                if self.device == torch.device(\"cuda:0\"):\n",
        "                  batchx = batchx.cuda(non_blocking=True)\n",
        "                  batchy[\"boxes\"] = batchy[\"boxes\"].cuda(non_blocking=True)\n",
        "                  batchy[\"image_class\"] = batchy[\"image_class\"].cuda(non_blocking=True)\n",
        "                  batchy[\"is_tree\"] = batchy[\"is_tree\"].cuda(non_blocking=True)\n",
        "                class_labels = batchy[\"image_class\"]\n",
        "                box_labels = batchy[\"boxes\"]\n",
        "                is_tree_labels = batchy[\"is_tree\"]\n",
        "                is_tree_preds, box_preds = self.model.forward(batchx)\n",
        "                losses.append(self._loss_specification(is_tree_labels, is_tree_preds, box_labels, box_preds).data)\n",
        "                class_accs.append(float((torch.round(is_tree_preds) == is_tree_labels.squeeze()).sum())/self.val_data_loader.batch_size)\n",
        "                ious.append(torch.mean(torch.as_tensor([iou(box_labels[i, :], box_preds[i, :]) for i in range(box_labels.size()[0])], dtype=torch.float32)))\n",
        "                rmses.append(rmse(box_preds, box_labels))\n",
        "\n",
        "              \n",
        "            losses = torch.mean(torch.as_tensor(losses, dtype=torch.float32))\n",
        "            class_accs = torch.mean(torch.as_tensor(class_accs, dtype=torch.float32))\n",
        "            box_rmse = torch.mean(torch.as_tensor(rmses, dtype=torch.float32))\n",
        "            avg_box_iou = torch.mean(torch.as_tensor(ious, dtype=torch.float32))\n",
        "            val_epoch_loss.append(losses)\n",
        "            val_epoch_acc.append(class_accs)\n",
        "            val_epoch_rmse.append(box_rmse)\n",
        "            val_epoch_iou.append(avg_box_iou)\n",
        "\n",
        "            # We can change this to be epoch wise or not averaged over all batches\n",
        "            print (\"Batch Average Val Loss: {:.3f}\".format(losses))\n",
        "            print (\"Batch Avg Val Classification Acc: {:.3f}\".format(class_accs))\n",
        "            print (\"Batch Avg Val BBox RMSE: {:.3f}\".format(box_rmse))\n",
        "            print (\"Batch Avg Avg Bbox IoU: {:.3f} \\n\".format(avg_box_iou))\n",
        "          self.model.train()\n",
        "      epoch_loss.append(torch.mean(torch.as_tensor(batch_loss, dtype=torch.float32)))\n",
        "      epoch_acc.append(torch.mean(torch.as_tensor(batch_acc, dtype=torch.float32)))\n",
        "      epoch_iou.append(torch.mean(torch.as_tensor(batch_iou, dtype=torch.float32)))\n",
        "      epoch_rmse.append(torch.mean(torch.as_tensor(batch_rmse, dtype=torch.float32)))\n",
        "  \n",
        "      print (\"Epoch \", epoch + 1, \" finished in \", time.time() - epoch_start)\n",
        "    tr_metric_dict = {\"Loss\": epoch_loss, \"Acc\": epoch_acc, \"IoU\": epoch_iou, \"RMSE\": epoch_rmse}\n",
        "    val_metric_dict = {\"Loss\": val_epoch_loss, \"Acc\": val_epoch_acc, \"IoU\": val_epoch_iou, \"RMSE\": val_epoch_rmse}\n",
        "    torch.save({\n",
        "              'epoch': epoch + 1,\n",
        "              'model_state_dict': self.model.state_dict(),\n",
        "              'optimizer_state_dict': self.optimizer.state_dict(),\n",
        "              'tr_metric_dict': tr_metric_dict, \n",
        "              'val_metric_dict': val_metric_dict\n",
        "              }, \n",
        "            self.model_savepath)\n",
        "    print (\"Final checkpoint created. Model dict and metrics saved. \")\n",
        "    return self.model, tr_metric_dict, val_metric_dict\n",
        "\n",
        "\n",
        "  def _loss_specification(self, is_tree_labels, is_tree_preds, box_labels, box_preds):\n",
        "    binary_detection_error = self.binary_classification_criterion(is_tree_preds, is_tree_labels.unsqueeze(1)) # output, target\n",
        "    bounding_box_error = self.regression_criterion(box_preds, box_labels)\n",
        "    return self.alpha * binary_detection_error + self.beta * bounding_box_error"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U6TrQ9DDL6HD",
        "outputId": "a08f9a6a-9289-4f2f-af9e-6574010dcc2a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        }
      },
      "source": [
        "torch.multiprocessing.set_start_method(\"spawn\", force=True)\n",
        "\n",
        "_N_WORKERS = 0\n",
        "_PIN_MEM = True\n",
        "\n",
        "mob_model = Customized_MobileNet(pretrained_model=mobilenet)\n",
        "trainer = ModelTrainer(mob_model, \n",
        "                       data_set, \n",
        "                       train_split=0.8,\n",
        "                       learning_rate=0.002,\n",
        "                       batch_size = 64,\n",
        "                       device=_DEVICE, \n",
        "                       pin_memory=_PIN_MEM,\n",
        "                       n_workers=_N_WORKERS,\n",
        "                       model_savepath=model_path)\n",
        "print (trainer.describe_training())\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "33506  training examples\n",
            "8377  validation examples\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xl5ULX4vu4YB"
      },
      "source": [
        "### Check model structure and parameters\n",
        "Regressor and classifier (final layer) should require grad. Others should not. Optimizer should be set only to those regressor and classifier variables. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SdO1y3kTtfpM",
        "outputId": "ede72e29-e68c-4c91-bf85-728013fb0ed5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        }
      },
      "source": [
        "print (\"Regressor params\")\n",
        "print ([p for p in mob_model.regressor.parameters()])\n",
        "\n",
        "print (\"Classifier params\")\n",
        "print ([p for p in mob_model.classifier.parameters()])\n",
        "\n",
        "# Should output just two layers (4 variables total)\n",
        "for param in mob_model.parameters():\n",
        "  if param.requires_grad:\n",
        "    print (param.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Regressor params\n",
            "[Parameter containing:\n",
            "tensor([[-0.0255,  0.0035, -0.0125,  ..., -0.0158, -0.0085,  0.0181],\n",
            "        [-0.0224,  0.0162,  0.0235,  ...,  0.0194, -0.0209, -0.0268],\n",
            "        [ 0.0277, -0.0057, -0.0008,  ...,  0.0081,  0.0156,  0.0271],\n",
            "        [ 0.0049, -0.0244,  0.0116,  ...,  0.0036,  0.0191,  0.0244]],\n",
            "       device='cuda:0', requires_grad=True), Parameter containing:\n",
            "tensor([-0.0066, -0.0188, -0.0213,  0.0067], device='cuda:0',\n",
            "       requires_grad=True)]\n",
            "Classifier params\n",
            "[Parameter containing:\n",
            "tensor([[-0.0001,  0.0109, -0.0252,  ..., -0.0123,  0.0037,  0.0106]],\n",
            "       device='cuda:0', requires_grad=True), Parameter containing:\n",
            "tensor([0.0167], device='cuda:0', requires_grad=True)]\n",
            "torch.Size([1, 1280])\n",
            "torch.Size([1])\n",
            "torch.Size([4, 1280])\n",
            "torch.Size([4])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72xHKgN4RLah",
        "outputId": "e23058b9-cd65-4c19-e65e-2cbe1913d8e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 409
        }
      },
      "source": [
        "trainer.train(num_epochs=10, batch_report=50) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting at epoch 0\n",
            "==================================================\n",
            "EPOCH  0\n",
            "\n",
            "Last 10 Batch Avg Metrics, Batch 50/524\n",
            "Total Loss: 37613.637\n",
            "Classification Acc: 2.925\n",
            "BBox RMSE: 542.872\n",
            "Avg Bbox IoU: 0.017 \n",
            "\n",
            "Checkpoint created\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/TiffImagePlugin.py:788: UserWarning: Corrupt EXIF data.  Expecting to read 4 bytes but only got 0. \n",
            "  warnings.warn(str(msg))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Last 10 Batch Avg Metrics, Batch 100/524\n",
            "Total Loss: 30223.746\n",
            "Classification Acc: 0.105\n",
            "BBox RMSE: 487.777\n",
            "Avg Bbox IoU: 0.114 \n",
            "\n",
            "Checkpoint created\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qrZ7fiLeRiw3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
>>>>>>> 26039cbb9be4bed73ea4ced92991de85923ab12d
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
